{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering the daily load profiles"
      ],
      "metadata": {
        "id": "2DjhCq0Iug4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and import necessary libraries"
      ],
      "metadata": {
        "id": "6tC-oxi2uo2v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdYEqdVfrASO",
        "outputId": "877caecd-1dd3-4cc2-8e05-28cf80e9584a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_inAhUArCJf",
        "outputId": "8a8fcc7f-88b3-4880-af18-5233a3461f91"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, date_format, min, max, hour, avg, to_timestamp\n",
        "from pyspark.sql.types import StructType\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os"
      ],
      "metadata": {
        "id": "AXQZmB06rE0F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define necessary functions"
      ],
      "metadata": {
        "id": "nTY5oAe3uuAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_csv(spark, data_path):\n",
        "\n",
        "    # Read CSV data\n",
        "    df = spark.read.csv(data_path, header=True)\n",
        "\n",
        "    # Cast PowerFlowValue to float and Timestamp to timestamp type\n",
        "    df = df.withColumn('PowerFlowValue', col('PowerFlowValue').cast('float'))\n",
        "    df = df.withColumn('Timestamp', to_timestamp(col('Timestamp'), 'yyyy-MM-dd HH:mm:ss'))\n",
        "\n",
        "    # Select relevant feature for normalization\n",
        "    assembler = VectorAssembler(inputCols=[\"PowerFlowValue\"], outputCol=\"features\")\n",
        "    df = assembler.transform(df)\n",
        "\n",
        "    # Initialize the MinMaxScaler\n",
        "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"NormalizedPowerFlowValue\")\n",
        "\n",
        "    # Compute summary statistics and generate the MinMaxScalerModel\n",
        "    scaler_model = scaler.fit(df)\n",
        "\n",
        "    # Transform the data\n",
        "    df_normalized = scaler_model.transform(df)\n",
        "\n",
        "    # Convert vector column to array\n",
        "    df_normalized = df_normalized.withColumn(\"NormalizedPowerFlowArray\", vector_to_array(\"NormalizedPowerFlowValue\"))\n",
        "\n",
        "    # Extract individual values from the array into separate columns\n",
        "    df_normalized = df_normalized.withColumn(\"NormalizedPowerFlowValue\", col(\"NormalizedPowerFlowArray\")[0])\n",
        "\n",
        "    # Drop the intermediate array column\n",
        "    df_normalized = df_normalized.drop(\"NormalizedPowerFlowArray\")\n",
        "\n",
        "    # Group by hour and calculate the average power flow value for each hour\n",
        "    hourly_avg_df = (\n",
        "        df_normalized\n",
        "        .withColumn(\"Hour\", hour(\"Timestamp\"))\n",
        "        .groupBy(\"PowerLineID\", \"Hour\")\n",
        "        .agg(avg(\"NormalizedPowerFlowValue\").alias(\"AveragePowerFlow\"))\n",
        "    )\n",
        "\n",
        "    # Sort the resulting DataFrame\n",
        "    hourly_avg_df = hourly_avg_df.sort('PowerLineID', 'Hour')\n",
        "\n",
        "    return hourly_avg_df"
      ],
      "metadata": {
        "id": "4Oua6ZTUrIqV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_train_model(data_path, save_path):\n",
        "    # Create a Spark session\n",
        "    spark = SparkSession.builder.appName(\"Clustering\").getOrCreate()\n",
        "\n",
        "    # Process CSV\n",
        "    processed_df = process_csv(spark, data_path)\n",
        "\n",
        "    # Specify the columns to be used as features\n",
        "    feature_columns = ['AveragePowerFlow']\n",
        "\n",
        "    # Create a vector assembler\n",
        "    vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "    # Transform the DataFrame to include the features vector\n",
        "    feature_df = vector_assembler.transform(processed_df)\n",
        "\n",
        "    # Define the number of clusters\n",
        "    num_clusters = 50\n",
        "\n",
        "    # Initialize the KMeans model\n",
        "    kmeans = KMeans(featuresCol=\"features\", predictionCol=\"cluster\", k=num_clusters, seed=1)\n",
        "\n",
        "    # Fit the model to the data\n",
        "    model = kmeans.fit(feature_df)\n",
        "\n",
        "    # Save the model\n",
        "    model.save(save_path)"
      ],
      "metadata": {
        "id": "-9J3YPXasDBF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and save models"
      ],
      "metadata": {
        "id": "mbex8z2ruxk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process and train models for all CSV files\n",
        "data_folder = '/content/drive/My Drive/Power_flow_data'\n",
        "models_save_folder = '/content/drive/My Drive/kmeans_models'\n",
        "\n",
        "# Iterate over all CSV files\n",
        "for csv_file in os.listdir(data_folder):\n",
        "    if csv_file.endswith(\".csv\"):\n",
        "        # Create full path for CSV and model save\n",
        "        csv_path = os.path.join(data_folder, csv_file)\n",
        "        model_save_path = os.path.join(models_save_folder, f\"model_{csv_file.replace('.csv', '')}\")\n",
        "\n",
        "        # Process and train model\n",
        "        process_and_train_model(csv_path, model_save_path)\n",
        "        print(\"Model for \" + str(csv_file) + \" trained and saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY8Tpi0wrJec",
        "outputId": "1b2e5483-483b-4ad5-be2f-cc8382b6facd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model for 2010-07-02.csv trained and saved!\n",
            "Model for 2010-07-01.csv trained and saved!\n",
            "Model for 2010-07-03.csv trained and saved!\n",
            "Model for 2010-07-04.csv trained and saved!\n",
            "Model for 2010-07-05.csv trained and saved!\n",
            "Model for 2010-07-06.csv trained and saved!\n",
            "Model for 2010-07-07.csv trained and saved!\n",
            "Model for 2010-07-08.csv trained and saved!\n",
            "Model for 2010-07-09.csv trained and saved!\n",
            "Model for 2010-07-10.csv trained and saved!\n",
            "Model for 2010-07-11.csv trained and saved!\n",
            "Model for 2010-07-12.csv trained and saved!\n",
            "Model for 2010-07-13.csv trained and saved!\n",
            "Model for 2010-07-14.csv trained and saved!\n",
            "Model for 2010-07-15.csv trained and saved!\n",
            "Model for 2010-07-16.csv trained and saved!\n",
            "Model for 2010-07-17.csv trained and saved!\n",
            "Model for 2010-07-18.csv trained and saved!\n",
            "Model for 2010-07-19.csv trained and saved!\n",
            "Model for 2010-07-20.csv trained and saved!\n",
            "Model for 2010-07-21.csv trained and saved!\n",
            "Model for 2010-07-22.csv trained and saved!\n",
            "Model for 2010-07-23.csv trained and saved!\n",
            "Model for 2010-07-24.csv trained and saved!\n",
            "Model for 2010-07-25.csv trained and saved!\n",
            "Model for 2010-07-26.csv trained and saved!\n",
            "Model for 2010-07-27.csv trained and saved!\n",
            "Model for 2010-07-28.csv trained and saved!\n",
            "Model for 2010-07-29.csv trained and saved!\n",
            "Model for 2010-07-30.csv trained and saved!\n",
            "Model for 2010-07-31.csv trained and saved!\n",
            "Model for 2010-08-01.csv trained and saved!\n",
            "Model for 2010-08-02.csv trained and saved!\n",
            "Model for 2010-08-03.csv trained and saved!\n",
            "Model for 2010-08-04.csv trained and saved!\n",
            "Model for 2010-08-05.csv trained and saved!\n",
            "Model for 2010-08-06.csv trained and saved!\n",
            "Model for 2010-08-07.csv trained and saved!\n",
            "Model for 2010-08-08.csv trained and saved!\n",
            "Model for 2010-08-09.csv trained and saved!\n",
            "Model for 2010-08-10.csv trained and saved!\n",
            "Model for 2010-08-11.csv trained and saved!\n",
            "Model for 2010-08-12.csv trained and saved!\n",
            "Model for 2010-08-13.csv trained and saved!\n",
            "Model for 2010-08-14.csv trained and saved!\n",
            "Model for 2010-08-15.csv trained and saved!\n",
            "Model for 2010-08-16.csv trained and saved!\n",
            "Model for 2010-08-17.csv trained and saved!\n",
            "Model for 2010-08-18.csv trained and saved!\n",
            "Model for 2010-08-19.csv trained and saved!\n",
            "Model for 2010-08-20.csv trained and saved!\n",
            "Model for 2010-08-21.csv trained and saved!\n",
            "Model for 2010-08-22.csv trained and saved!\n",
            "Model for 2010-08-23.csv trained and saved!\n",
            "Model for 2010-08-24.csv trained and saved!\n",
            "Model for 2010-08-25.csv trained and saved!\n",
            "Model for 2010-08-26.csv trained and saved!\n",
            "Model for 2010-08-27.csv trained and saved!\n",
            "Model for 2010-08-28.csv trained and saved!\n",
            "Model for 2010-08-29.csv trained and saved!\n",
            "Model for 2010-08-30.csv trained and saved!\n",
            "Model for 2010-08-31.csv trained and saved!\n",
            "Model for 2010-09-01.csv trained and saved!\n",
            "Model for 2010-09-02.csv trained and saved!\n",
            "Model for 2010-09-03.csv trained and saved!\n",
            "Model for 2010-09-04.csv trained and saved!\n",
            "Model for 2010-09-05.csv trained and saved!\n",
            "Model for 2010-09-06.csv trained and saved!\n",
            "Model for 2010-09-07.csv trained and saved!\n",
            "Model for 2010-09-08.csv trained and saved!\n",
            "Model for 2010-09-09.csv trained and saved!\n",
            "Model for 2010-09-10.csv trained and saved!\n",
            "Model for 2010-09-11.csv trained and saved!\n",
            "Model for 2010-09-12.csv trained and saved!\n",
            "Model for 2010-09-13.csv trained and saved!\n",
            "Model for 2010-09-14.csv trained and saved!\n",
            "Model for 2010-09-15.csv trained and saved!\n",
            "Model for 2010-09-16.csv trained and saved!\n",
            "Model for 2010-09-17.csv trained and saved!\n",
            "Model for 2010-09-18.csv trained and saved!\n",
            "Model for 2010-09-19.csv trained and saved!\n",
            "Model for 2010-09-20.csv trained and saved!\n",
            "Model for 2010-09-21.csv trained and saved!\n",
            "Model for 2010-09-22.csv trained and saved!\n",
            "Model for 2010-09-23.csv trained and saved!\n",
            "Model for 2010-09-24.csv trained and saved!\n",
            "Model for 2010-09-25.csv trained and saved!\n"
          ]
        }
      ]
    }
  ]
}